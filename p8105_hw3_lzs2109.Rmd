---
title: "Data Science I Homework 3 - lzs2109"
author: "Louis Sharp"
date: "10/12/2021"
output: github_document
---

### **Problem 1**

```{r, message = FALSE}
library(tidyverse)
library(p8105.datasets)
```

```{r}
data("instacart")
```

This instacart dataset includes a bunch of information about orders placed on the online grocery delivery service Instacart. Variables include `r names(instacart)`, which describe unique identifiers used by the app and for products, orders, aisles, departments, and users. In addition, information on how many of the unique items were added to carts, reordered, what day of the week they were ordered, what hour of the day, and how long since the last order of that particular item are inlcuded. Finally, product information including the department and aisle in which they are located are available, as well as the specific product names. This dataset contains a lot of information, with `r nrow(instacart)` observations and `r ncol(instacart)` variables.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  arrange(desc(n))
```

There are 134 different aisles in the dataset, with over 150,000 items ordered from both the fresh vegetables and fresh fruits aisles. The next most ordered from aisle is the packaged vegetables fruits aisle with almost 78,500 items being ordered from it.


Now, here's a plot showing the number of items ordered in each aisle, with only aisles with over 10,000 items included.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
  ggplot(aes(y = aisle, x = n_obs)) + 
  geom_col() +
  labs(title = "Items Ordered Per Aisle on Instacart", 
       x = "Number of Items Ordered", 
       y = "Aisle Name")
```

Next, we'll look at the top 3 most popular items from the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits".

```{r}
instacart %>% 
  filter(aisle == c("baking ingredients",
                    "dog food care",
                    "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(times_ordered = n()) %>% 
  mutate(product_rank = min_rank(desc(times_ordered))) %>% 
  filter(product_rank < 4) %>% 
  arrange(aisle, product_rank) %>% 
  relocate(product_rank, product_name, times_ordered, aisle) %>% 
  knitr::kable()
```

Next, let's explore what mean hour of the day Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week, with a column for each day of the week and a row for each of the two items.

```{r}
instacart %>% 
  select(order_hour_of_day, order_dow, product_name) %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(mean_hour = as.integer(mean_hour)) %>% 
  pivot_wider(names_from = order_dow, values_from = mean_hour) %>% 
  knitr::kable()
```


### **Problem 2**

```{r}
data("brfss_smart2010")

brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr,
         county = locationdesc,
         resp_id = respid) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, 
                           ordered = TRUE, 
                           levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))

#filtering by "Overall Health" seems to have eliminated all responses other
#than those from Poor to Excellent, so no additional code for that is needed.
```

For the years 2002 and 2010, we want to determine which states were observed at 7 or more locations, renamed to "county" in this dataset. Below, we'll investigate that.

```{r}
brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  select(year, state, county) %>% 
  group_by(year, state) %>% 
  distinct() %>% 
  summarize(n_county = n()) %>% 
  filter(n_county > 6)
```

It looks like in 2002, only six states were observed at 7 or more locations or counties. These states included Connecticut (7 counties), Florida (7 counties), Massachusets (8 counties), North Carolina (7 counties), New Jersey (8 counties), and Pennsylvania (10 counties). None were observed at more than 10 counties or locations.

```{r}
brfss_smart2010 %>% 
  filter(year == 2010) %>% 
  select(year, state, county) %>% 
  group_by(year, state) %>% 
  distinct() %>% 
  summarize(n_county = n()) %>% 
  filter(n_county > 6)
```

By 2010, fourteen different states were observed at 7 or more locations. These states include five of the six from 2002 (no Connecticut), as well as California, Colorado, Maryland, Nebraska, New York, Ohio, South Carolina, Texas, and Washington.

```{r}
brfss_smart2010 %>% 
  filter(response == "Excellent") %>% 
  select(year, state, county, data_value) %>% 
  group_by(year, state, county) %>% 
  summarize(mean_data_value = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_line()
```

```{r}
brfss_smart2010 %>% 
  filter(state == "NY",
         year == 2006 | year == 2010) %>% 
  select(year, state, county, response, data_value) %>% 
  ggplot(aes(x = response, y = data_value, color = county)) + 
  geom_boxplot() +
  facet_grid(. ~ year)
```


### **Problem 3**

```{r}
accel_df = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440, 
               names_to = "minute",
               names_prefix = "activity_",
               values_to = "activity_level") %>% 
  mutate(part_of_week = ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday"),
  minute = as.numeric(minute))
```

This tidied accelerometer dataset includes the following variables: `r names(accel_df)` with an observation for every minute of every day over a 5 week period. If you didn't feel like doing the math, that's a total of `r nrow(accel_df)` observations of accelerometer filled fun! But seriously, this dataset is important information because it follows a man in his 60s with a normal range BMI who was diagnosed with congestive heart failure, and thus around-the-clock observations of his activity level were important for health and medical purposes. As such, the dataset is very precise, broken down by a numerical activity level for every minute of every day for the whole 5 week period, with variables identifying what day of the week and whether it was a weekday or weekend for precise analysis.

```{r}
accel_df = accel_df %>% 
  group_by(day_id) %>% 
  mutate(total_activity = sum(activity_level))

accel_df %>% 
  group_by(day_id, day, week, total_activity) %>% 
  summarize() %>% 
  view()
```

There aren't any striking trends across the give weeks that really jump out. Mid-week seems to be relatively less busy (Tues-Thurs) throughout the 5 week period, and for the first three weeks, when there's one busy day on the weekend, the other seems relatively less busy. The two Saturdays in weeks 4 and 5, nothing seems to be happening at all, which is probably the most striking feature of this aggregated activity dataset. The highest levels of activity for each week seem to happen between Friday-Monday, with the exception of week 4.

```{r}
accel_df %>% 
  ggplot(aes(x = day_id, y = total_activity, color = day)) +
  geom_point() +
  geom_line()
```

