---
title: "Data Science I Homework 3 - lzs2109"
author: "Louis Sharp"
date: "10/12/2021"
output: github_document
---

### **Problem 1**

```{r, message = FALSE}
library(tidyverse)
library(p8105.datasets)
```

```{r}
data("instacart")
```

This instacart dataset includes a bunch of information about orders placed on the online grocery delivery service Instacart. Variables include `r names(instacart)`, which describe unique identifiers used by the app and identifiers for products, orders, aisles, departments, and users. In addition, information on how many of the unique items were added to carts, reordered, what day of the week they were ordered, what hour of the day, and how long since the last order of that particular item are inlcuded. Finally, product information including the department and aisle in which they are located are available, as well as the specific product names. This dataset contains a lot of information, with `r nrow(instacart)` observations (over one and a quarter million!) and `r ncol(instacart)` variables.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  arrange(desc(n))
```

There are 134 different aisles in the dataset, with over 150,000 items ordered from both the fresh vegetables and fresh fruits aisles. The next most ordered from aisle is the packaged vegetables fruits aisle with almost 78,500 items being ordered from it. The yogurt aisle is the next and 4th most ordered from with over 55,000 items coming from it. The top 5 most ordered from aisles is rounded out with packaged cheese, at one order shy of 41,700 orders.


Now, here's a plot showing the number of items ordered in each aisle, showing only aisles with over 10,000 items ordered. We can clearly see the top 5 listed above, with water seltzer sparkling water coming in closely at 6th, followed by milk at 7th, as in our list above. Orders from the candy chocolate, dry pasta, oils vinegars, and butter aisles seem to be just past the 10,000 orders mark, making them still quite popular, but rounding out the bottom of the 10k+ list.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
  ggplot(aes(y = reorder(aisle, n_obs), x = n_obs)) + 
  geom_col() +
  labs(title = "Items Ordered Per Aisle on Instacart", 
       x = "Number of Items Ordered", 
       y = "Aisle Name")
```

Next, we'll look at the top 3 most popular items from the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits".

```{r}
instacart %>% 
  filter(aisle == c("baking ingredients",
                    "dog food care",
                    "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(times_ordered = n()) %>% 
  mutate(product_rank = min_rank(desc(times_ordered))) %>% 
  filter(product_rank < 4) %>% 
  arrange(aisle, product_rank) %>% 
  relocate(product_rank, aisle, product_name, times_ordered) %>% 
  knitr::kable()
```

Brown sugar seems to be the most commonly ordered item from the baking ingredients aisle, followed by baking soda and organic vanilla extract. I suppose a lot of people are baking cookies and cakes! The dog food care aisle doesn't seem to get a ton of orders on instacart, as the most common items are ordered less than 15 times. Those include "Organix" dog foods and something called Original Dry Dog..? Most likely a type of dry kibble dog food. Finally, the packaged vegetables fruits aisle is quite popular, as we saw above, and gets thousands of orders for organic spinach mostly, followed by organic berries.


Next, let's explore what mean hour of the day Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week, with a column for each day of the week and a row for each of the two items.

```{r}
instacart %>% 
  select(order_hour_of_day, order_dow, product_name) %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(mean_hour = round(mean_hour),
         mean_hour = as.integer(mean_hour),
         days_of_week = c("Sunday", "Monday", "Tuesday", 
                          "Wednesday", "Thursday", "Friday", "Saturday")) %>% 
  select(-order_dow) %>% 
  pivot_wider(
    names_from = days_of_week, 
    values_from = mean_hour) %>% 
  knitr::kable(caption = "**Mean Times (24hr) Ordered by Day**")
```

This table has the hours rounded off, seeing as how in the original table the numbers were formatted as base ten decimals, which gave a result that didn't look like times. In addition, not rounding off made hours like 11.83 and 11.33 both appear as 11, which also doesn't seem to accurately reflect the average hour of the orders. As such, people seem to be ordering coffee ice cream later in the day, primarily in the afternoons (except Fridays), perhaps for an extra (and sweet) boost of caffeine late in the day. Or maybe they just plan to watch Netflix and eat coffee ice cream when they get home from work in the afternoon. Pink lady apples, on the other hand, seem to be ordered mostly around mid-day.


### **Problem 2**

```{r}
data("brfss_smart2010")
```

```{r}
brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr,
         county = locationdesc,
         resp_id = respid) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, 
                           ordered = TRUE, 
                           levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))

#filtering by "Overall Health" seems to have eliminated all responses other
#than those from Poor to Excellent, so no additional code for that is needed.

brfss_smart2010 #outputting table since it was saved as a tidied table above
```

For the years 2002 and 2010, we want to determine which states were observed at 7 or more locations, renamed to "county" in this dataset. Below, we'll investigate that.

```{r}
brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  select(year, state, county) %>% 
  group_by(year, state) %>% 
  distinct() %>% 
  summarize(n_county = n()) %>% 
  filter(n_county > 6)
```

It looks like in 2002, only six states were observed at 7 or more locations/counties. These states included Connecticut (7 counties), Florida (7 counties), Massachusets (8 counties), North Carolina (7 counties), New Jersey (8 counties), and Pennsylvania (10 counties). None were observed at more than 10 counties or locations.

```{r}
brfss_smart2010 %>% 
  filter(year == 2010) %>% 
  select(year, state, county) %>% 
  group_by(year, state) %>% 
  distinct() %>% 
  summarize(n_county = n()) %>% 
  filter(n_county > 6)
```

By 2010, fourteen different states were observed at 7 or more locations. These states include five of the six from 2002 (not Connecticut), as well as California, Colorado, Maryland, Nebraska, New York, Ohio, South Carolina, Texas, and Washington. All of the states that appeared on the 2002 list and the 2010 list were observed at more locations in 2010 with the exception of Pennsylvania, which dropped from 10 locations in 2002 to 7 in 2010. Please see the list for number of locations observed for each state.

```{r}
brfss_smart2010 %>% 
  filter(response == "Excellent") %>% 
  select(year, state, county, data_value) %>% 
  group_by(year, state) %>% 
  summarize(mean_data_value = mean(data_value)) %>%
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_line(aes(group = state), alpha = 0.6) + #not sure if the group aesthetic made a difference..
  labs(title = "Average 'Excellent' Response Data Value Over Time Per State",
       y = "Average Data Value", x = "Year")
```

The spaghetti plot is pretty busy so it's difficult to discern any useful information from it for individual states. Overall though, the range of average data values across states ranges from about 17 on the low end to about 29 on the high end, with most states being between 20 and 25. It seems like most states, if not all, start off with higher averages in 2002 and end with lower averages in 2010, meaning that the average data value of "Excellent" responses goes down over this 8 year period from 2002 to 2010. This could be attributable to the fact that, as mentioned above, many states had been observed in more locations in 2010 than in 2002, adding more data points that could have brought the average data value down if, for example, the new locations were in areas where people with "Excellent" health responses were rated lower than in the previous locations observed.

```{r}
brfss_smart2010 %>% 
  filter(state == "NY",
         year == 2006 | year == 2010) %>% 
  select(year, state, county, response, data_value) %>% 
  ggplot(aes(x = data_value, color = response, fill = response)) + 
  geom_density(alpha = .3) +
  labs(title = "Distribution of Data Values By Response in NY State",
       x = "Data Value", y = "Density/Frequency") +
  xlim(-1, 50) +
  theme(legend.position = "bottom") +
  facet_grid(. ~ year)
```

Above is a plot showing the distribution of data values for the ordered responses from "Poor" to "Excellent" among locations in New York state in 2006 and 2010. As we can see, the "Poor" responses are all clustered tightly between data values 0-5 for 2006, and a little more spread in 2010 between data values 0-8 or so, but still tightly clustered. The height of the peaks shows that many of the values for the "Poor" response are in the same small window. In 2006, the "Fair" responses also encompass a smaller range than in 2010, where the peak smooths out a little and also captures a wider range of data values. Interestingly, the "Excellent" responses for both years appear to capture values more in the range between "Fair" and "Good"/"Very good". This seems unusual, but consistent across both years. The "Good" values for both years appear consistently higher, in general, than those of "Excellent" responses, with a lot of overlap between the high range of "Excellent", the low range of "Very good" and the overall "Good" responses in 2006. They seem more separate in 2010, but still with a considerable amount of overlap. Finally, the "Very good" responses seems to have a higher distribution of high data values in 2006 than any other response, and this becomes more apparent in the 2010 data. There is however, as mentioned above, a lot of overlap between these three highest responses, but still a demarcation where the "Excellent" responses seem to have lower values than the "Good" responses, which in turn have lower values than "Very good" responses, which have the highest values.


### **Problem 3**

```{r}
accel_df = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440, 
               names_to = "minute",
               names_prefix = "activity_",
               values_to = "activity_level") %>% 
  mutate(part_of_week = ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday"),
  minute = as.numeric(minute))

accel_df
```

This tidied accelerometer dataset includes the following variables: `r names(accel_df)` with an observation for every minute of every day over a 5 week period. If you didn't feel like doing the math, that's a total of `r nrow(accel_df)` observations of accelerometer filled fun! But seriously, this dataset is important information because it follows a man in his 60s with a normal range BMI who was diagnosed with congestive heart failure, and thus around-the-clock observations of his activity level were important for health and medical purposes. As such, the dataset is very precise, broken down by a numerical activity level for every minute of every day for the whole 5 week period, with variables identifying what day of the week and whether it was a weekday or weekend for precise analysis.

```{r}
accel_df = accel_df %>% 
  group_by(day_id) %>% 
  mutate(total_activity = sum(activity_level))

accel_df %>% 
  group_by(day_id, day, week, total_activity) %>% 
  summarize()
```

There aren't any striking trends across the five weeks that really jump out. Mid-week seems to be relatively less busy (Tues-Thurs) throughout the 5 week period, and for the first three weeks, when there's one busy day on the weekend, the other seems relatively less busy. The two Saturdays in weeks 4 and 5, nothing seems to be happening at all, which is probably the most striking feature of this aggregated activity dataset. The highest levels of activity for each week seem to happen between Friday-Monday, with the exception of week 4.

```{r}
accel_df %>% 
  ggplot(aes(x = week, y = total_activity, color = day)) +
  geom_point() +
  geom_path() +
  labs(title = "24-Hour Activity Time Courses For Each Day, by Week",
       x = "Week",
       y = "Total Activity")
```

I first made the above plot with day_id on the x-axis, but this layout seems to offer more clarity as we can still see the day by day breakdown, except by weeks instead of points and lines starting at different x-axis values based on day_id values. Based on this plot, we can see how activity level changes per day for each week, and for each day over weeks. For example, in week 1, Monday is by far the least active day, then activity increases on Tuesday, then more on Wednesday, then more on Thursday. There's an incremental increase of activity by day for the first week, the only decrease is from Friday to Saturday, then a big jump of activity on Sunday. In week two, the activity levels are more tightly clustered, but with similar incremental activity across the week, except this time Saturday is the more active weekend day than Sunday. By week 3, the person being followed seems to be aiming for a similar activity level on each day, with the biggest jump happening on Monday, seemingly trying to start the week off strong. By week 4, Monday and Wednesday are the most active days, with all other weekdays being fairly high, but by Friday and the weekend, activity drops off. Maybe the sustained activity levels over the last two weeks have caught up with the individual and overwhelmed him, as by Friday, Saturday, and Sunday there is relatively less activity than in preceding weeks. In fact, there is no measured activity on Saturday. By week 5, the weekdays are active again, but the weekend activity levels fall fairly precipitously. This graphical interpretation and its layout provide a far better way to analyze the aggregated activity level data than the table produced above.